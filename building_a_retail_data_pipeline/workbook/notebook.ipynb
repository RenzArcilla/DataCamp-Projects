{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef36f535-4bdc-4e2b-a22a-179372324b26",
   "metadata": {},
   "source": [
    "![walmartecomm](walmartecomm.jpg)\n",
    "\n",
    "Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n",
    "\n",
    "In this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table in `PostgreSQL` database with the following features:\n",
    "\n",
    "# `grocery_sales`\n",
    "- `\"index\"` - unique ID of the row\n",
    "- `\"Store_ID\"` - the store number\n",
    "- `\"Date\"` - the week of sales\n",
    "- `\"Weekly_Sales\"` - sales for the given store\n",
    "\n",
    "Also, you have the `extra_data.parquet` file that contains complementary data:\n",
    "\n",
    "# `extra_data.parquet`\n",
    "- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n",
    "- `\"Temperature\"` - Temperature on the day of sale\n",
    "- `\"Fuel_Price\"` - Cost of fuel in the region\n",
    "- `\"CPI\"` â€“ Prevailing consumer price index\n",
    "- `\"Unemployment\"` - The prevailing unemployment rate\n",
    "- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n",
    "- `\"Dept\"` - Department Number in each store\n",
    "- `\"Size\"` - size of the store\n",
    "- `\"Type\"` - type of the store (depends on `Size` column)\n",
    "\n",
    "You will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n",
    "- `\"Store_ID\"`\n",
    "- `\"Month\"`\n",
    "- `\"Dept\"`\n",
    "- `\"IsHoliday\"`\n",
    "- `\"Weekly_Sales\"`\n",
    "- `\"CPI\"`\n",
    "- \"`\"Unemployment\"`\"\n",
    "\n",
    "After merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n",
    "\n",
    "|  Month | Weekly_Sales  | \n",
    "|---|---|\n",
    "| 1.0  |  33174.178494 |\n",
    "|  2.0 |  34333.326579 |\n",
    "|  ... | ...  |  \n",
    "\n",
    "Finally, you should save the `clean_data` and `agg_data` as the csv files.\n",
    "\n",
    "It is recommended to use `pandas` for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e758a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (20.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# # Install necessary packages\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0d64ff1-a4ca-4a82-a8b4-e210244dedc1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 99,
    "lastExecutedAt": 1751443504920,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport os\n\n# Extract function is already implemented for you \ndef extract(store_data, extra_data):\n    try:\n        extra_df = pd.read_parquet(extra_data)\n        merged_df = store_data.merge(extra_df, on = \"index\")\n        print(\"Merge complete.\")\n        return merged_df\n    except Exception as e:\n        print(e)\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Extract function is already implemented for you \n",
    "def extract(store_data, extra_data):\n",
    "    try:\n",
    "        store_data = pd.read_csv(store_data)\n",
    "        extra_df = pd.read_parquet(extra_data)\n",
    "        merged_df = store_data.merge(extra_df, on = \"index\")\n",
    "        print(\"Merge complete.\")\n",
    "        return merged_df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Call the extract() function and store it as the \"merged_df\" variable\n",
    "# Replaced the data from the database with a csv file for demonstration purposes\n",
    "merged_df = extract(\"sample_grocery_sales.csv\", \"extra_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "843226ee-3b30-4b58-bc85-7aa59fe53a83",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 50,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1751443504970,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Looking at the data\nprint(merged_df.head)",
    "outputsMetadata": {
     "0": {
      "height": 332,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          index  Store_ID        Date  Dept  Weekly_Sales  IsHoliday  \\\n",
      "0            0         1  2010-02-05     1      24924.50          0   \n",
      "1            1         1  2010-02-05    26      11737.12          0   \n",
      "2            2         1  2010-02-05    17      13223.76          0   \n",
      "3            3         1  2010-02-05    45         37.44          0   \n",
      "4            4         1  2010-02-05    28       1085.29          0   \n",
      "...        ...       ...         ...   ...           ...        ...   \n",
      "231517  232414        24  2011-05-06     8      49471.07          0   \n",
      "231518  232415        24  2011-05-06    50       1210.00          0   \n",
      "231519  232416        24  2011-05-06    87      25893.32          0   \n",
      "231520  232417        24  2011-05-06    85       1357.83          0   \n",
      "231521  232418        24  2011-05-06    35       3648.91          0   \n",
      "\n",
      "        Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  \\\n",
      "0             42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "1             42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "2             42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "3             42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "4             42.31       2.572        0.0        0.0        0.0        0.0   \n",
      "...             ...         ...        ...        ...        ...        ...   \n",
      "231517        55.75       4.192        0.0        0.0        0.0        0.0   \n",
      "231518        55.75       4.192        0.0        0.0        0.0        0.0   \n",
      "231519        55.75       4.192        0.0        0.0        0.0        0.0   \n",
      "231520        55.75       4.192        0.0        0.0        0.0        0.0   \n",
      "231521        55.75       4.192        0.0        0.0        0.0        NaN   \n",
      "\n",
      "        MarkDown5         CPI  Unemployment  Type      Size  \n",
      "0             0.0  211.096358         8.106   3.0  151315.0  \n",
      "1             0.0  211.096358         8.106   3.0  151315.0  \n",
      "2             0.0  211.096358         8.106   3.0  151315.0  \n",
      "3             0.0  211.096358           NaN   3.0  151315.0  \n",
      "4             0.0  211.096358           NaN   3.0  151315.0  \n",
      "...           ...         ...           ...   ...       ...  \n",
      "231517        0.0  134.514367         8.212   3.0  203819.0  \n",
      "231518        0.0  134.514367         8.212   3.0  203819.0  \n",
      "231519        0.0  134.514367         8.212   3.0  203819.0  \n",
      "231520        0.0  134.514367         8.212   3.0  203819.0  \n",
      "231521        NaN         NaN           NaN   NaN       NaN  \n",
      "\n",
      "[231522 rows x 17 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Looking at the data\n",
    "print(merged_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 48,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1751443505018,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n    try:\n        # Fill missing cells with their column mean values\n        # raw_data[['Weekly_Sales', 'CPI', 'Unemployment']].fillna(0, inplace=True)\n        raw_data.fillna(\n            {\n                'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n                'CPI': raw_data['CPI'].mean(),\n                'Unemployment': raw_data['Unemployment'].mean(),\n            },\n            inplace=True\n        )\n        print(\"Filled missing values\")\n\n        # Convert Date to datetime format\n        raw_data['Date'] = pd.to_datetime(raw_data['Date'], format='%Y-%m-%d')\n        print(\"Converted Date column to datetime type\")\n        # Create Month column\n        raw_data[\"Month\"] = raw_data['Date'].dt.month\n        print(\"Month column created.\")\n\n        # Drop rows with weekly sales $10,000 or less\n        raw_data = raw_data.loc[raw_data['Weekly_Sales'] > 10000, :]\n        print(\"Dropped rows with weekly sales < 10000\")\n        \n        # Filtering unnecessary columns\n        raw_data = raw_data[[\"Store_ID\", \"Month\", \"Dept\", \n                 \"IsHoliday\", \"Weekly_Sales\", \"CPI\", \"Unemployment\"]]\n        print(\"Filtered unnecessary columns\")\n\n        print(\"Transform complete.\")\n        print(raw_data.head)\n        return raw_data\n        \n    except Exception as e:\n        print(e)"
   },
   "outputs": [],
   "source": [
    "# Create the transform() function with one parameter: \"raw_data\"\n",
    "def transform(raw_data):\n",
    "    try:\n",
    "        # Fill missing cells with their column mean values\n",
    "        # raw_data[['Weekly_Sales', 'CPI', 'Unemployment']].fillna(0, inplace=True)\n",
    "        raw_data.fillna(\n",
    "            {\n",
    "                'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n",
    "                'CPI': raw_data['CPI'].mean(),\n",
    "                'Unemployment': raw_data['Unemployment'].mean(),\n",
    "            },\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # Convert Date to datetime format\n",
    "        raw_data['Date'] = pd.to_datetime(raw_data['Date'], format='%Y-%m-%d')\n",
    "        # Create Month column\n",
    "        raw_data[\"Month\"] = raw_data['Date'].dt.month\n",
    "\n",
    "        # Drop rows with weekly sales $10,000 or less\n",
    "        raw_data = raw_data.loc[raw_data['Weekly_Sales'] > 10000, :]\n",
    "        \n",
    "        # Filtering unnecessary columns\n",
    "        raw_data = raw_data[[\"Store_ID\", \"Month\", \"Dept\", \n",
    "                 \"IsHoliday\", \"Weekly_Sales\", \"CPI\", \"Unemployment\"]]\n",
    "\n",
    "        print(raw_data.head)\n",
    "        return raw_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "620b7289-06cd-4205-be9e-a50dc8d36cf0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 57,
    "lastExecutedAt": 1751443505075,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)",
    "outputsMetadata": {
     "0": {
      "height": 458,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         Store_ID  Month  Dept  IsHoliday  Weekly_Sales         CPI  \\\n",
      "0              1    2.0     1          0      24924.50  211.096358   \n",
      "1              1    2.0    26          0      11737.12  211.096358   \n",
      "2              1    2.0    17          0      13223.76  211.096358   \n",
      "5              1    2.0    79          0      46729.77  211.096358   \n",
      "6              1    2.0    55          0      21249.31  211.096358   \n",
      "...          ...    ...   ...        ...           ...         ...   \n",
      "231513        24    5.0    40          0      45396.26  134.514367   \n",
      "231515        24    5.0    93          0      41295.84  134.514367   \n",
      "231516        24    5.0     9          0      24024.18  134.514367   \n",
      "231517        24    5.0     8          0      49471.07  134.514367   \n",
      "231519        24    5.0    87          0      25893.32  134.514367   \n",
      "\n",
      "        Unemployment  \n",
      "0           8.106000  \n",
      "1           8.106000  \n",
      "2           8.106000  \n",
      "5           7.500052  \n",
      "6           7.500052  \n",
      "...              ...  \n",
      "231513      8.212000  \n",
      "231515      8.212000  \n",
      "231516      8.212000  \n",
      "231517      8.212000  \n",
      "231519      8.212000  \n",
      "\n",
      "[106231 rows x 7 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Call the transform() function and pass the merged DataFrame\n",
    "clean_data = transform(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b19b15e3-6624-47a9-927f-d3f12fe8212d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1751443505122,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n    try:\n        # Slicing columns\n        clean_data = clean_data[[\"Month\", \"Weekly_Sales\"]]\n        print(\"done column slice\")\n        \n        # Grouping by month, finding the avg weekly sales, reseting the index, rounding to 2 decimals\n        clean_data = clean_data.groupby(\"Month\").agg({'Weekly_Sales': 'mean'}).reset_index().round(2)\n        print(\"done group by month and finding avg\")\n\n        # Renaming weekly_sales column\n        clean_data.rename(columns={'Weekly_Sales': 'Avg_Sales'}, inplace=True)\n        \n        print(clean_data.head())\n\n        print(\"Avg monthly sales complete.\")\n        return clean_data\n    except Exception as e:\n        print(e)"
   },
   "outputs": [],
   "source": [
    "# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\n",
    "def avg_weekly_sales_per_month(clean_data):\n",
    "    try:\n",
    "        # Slicing columns\n",
    "        clean_data = clean_data[[\"Month\", \"Weekly_Sales\"]]\n",
    "        \n",
    "        # Grouping by month, finding the avg weekly sales, reseting the index, rounding to 2 decimals\n",
    "        clean_data = clean_data.groupby(\"Month\").agg({'Weekly_Sales': 'mean'}).reset_index().round(2)\n",
    "\n",
    "        # Renaming weekly_sales column\n",
    "        clean_data.rename(columns={'Weekly_Sales': 'Avg_Sales'}, inplace=True)\n",
    "        \n",
    "        print(clean_data)\n",
    "        return clean_data\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe875e27-b0cf-4e52-994e-4ae1fe6e8876",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1751443505170,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)",
    "outputsMetadata": {
     "0": {
      "height": 206,
      "type": "stream"
     },
     "1": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "ed8f2397-ee28-4e87-87f8-c9dc79e9d1e5",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Month  Avg_Sales\n",
      "0     1.0   33174.18\n",
      "1     2.0   34333.33\n",
      "2     3.0   33220.89\n",
      "3     4.0   33392.37\n",
      "4     5.0   33339.89\n",
      "5     6.0   34582.47\n",
      "6     7.0   33922.76\n",
      "7     8.0   33644.79\n",
      "8     9.0   33258.05\n",
      "9    10.0   32736.99\n",
      "10   11.0   36594.03\n",
      "11   12.0   39238.80\n"
     ]
    }
   ],
   "source": [
    "# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\n",
    "agg_data = avg_weekly_sales_per_month(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "921cb123-3153-4334-bdeb-9bb227fdc530",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1751443505220,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n    # Write your code here\n    try:\n        # Load full data to csv\n        full_data.to_csv(full_data_file_path, index=False)\n        print(f'Full data loaded to: {full_data_file_path}')\n\n        #Load agg. data to csv\n        agg_data.to_csv(agg_data_file_path, index=False)\n        print(f'Agg. data loaded to: {agg_data_file_path}')\n        \n    except Exception as e:\n        print(e)"
   },
   "outputs": [],
   "source": [
    "# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\n",
    "def load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n",
    "    # Write your code here\n",
    "    try:\n",
    "        # Load full data to csv\n",
    "        full_data.to_csv(full_data_file_path, index=False)\n",
    "        print(f'Full data loaded to: {full_data_file_path}')\n",
    "\n",
    "        #Load agg. data to csv\n",
    "        agg_data.to_csv(agg_data_file_path, index=False)\n",
    "        print(f'Agg. data loaded to: {agg_data_file_path}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f518ad5c-214e-474b-80bd-827b0c0e1536",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 471,
    "lastExecutedAt": 1751443505691,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths  \nload(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data loaded to: clean_data.csv\n",
      "Agg. data loaded to: agg_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Call the load() function and pass the cleaned and aggregated DataFrames with their paths  \n",
    "load(clean_data, 'clean_data.csv', agg_data, 'agg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61b5f58a-70cb-40b3-bdbe-20b4079276e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1751443505738,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n    # Write your code here\n    try:\n        if os.path.exists(file_path):\n            print(f'File Path: {file_path} exists in the current directory!')\n        else:\n            print(f'File Path: {file_path} does not exist!')\n    except Exception as e:\n        print(e)"
   },
   "outputs": [],
   "source": [
    "# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\n",
    "def validation(file_path):\n",
    "    # Write your code here\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f'File Path: {file_path} exists in the current directory!')\n",
    "        else:\n",
    "            print(f'File Path: {file_path} does not exist!')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df1659ff-41c4-4a92-9812-80c6eaa02b90",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1751443505786,
    "lastExecutedByKernel": "cfe07025-4bfe-4153-9aec-55a7b926053d",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation('clean_data.csv')\nvalidation('agg_data.csv')",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: clean_data.csv exists in the current directory!\n",
      "File Path: agg_data.csv exists in the current directory!\n"
     ]
    }
   ],
   "source": [
    "# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\n",
    "validation('clean_data.csv')\n",
    "validation('agg_data.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
